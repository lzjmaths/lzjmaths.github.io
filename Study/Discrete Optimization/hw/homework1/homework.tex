% Sample tex file for usage of iidef.sty
% Homework template for Inference and Information
% UPDATE: October 12, 2017 by Xiangxiang
% UPDATE: 22/03/2018 by zhaofeng-shu33
\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
% amsmath: equation*, amssymb: mathbb, amsthm: proof
\usepackage{moreenum}
\usepackage{mathtools}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % toprule
\usepackage[mathcal]{eucal}
\usepackage{dsfont}

\usepackage{setspace}  
\setstretch{1.6}








\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}[definition]{Example}
\newtheorem{exercise}[definition]{Exercise}
\newtheorem{remark}[definition]{Remark}
\newtheorem{observation}[definition]{Observation}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{convention}[definition]{Convention}
\newtheorem{priniple}[definition]{Principle}
\newtheorem{notation}[definition]{Notation}
\newtheorem*{axiom}{Axiom}
\newtheorem{coa}[definition]{Theorem}
\newtheorem{srem}[definition]{$\star$ Remark}
\newtheorem{seg}[definition]{$\star$ Example}
\newtheorem{sexe}[definition]{$\star$ Exercise}
\newtheorem{sdf}[definition]{$\star$ Definition}
\newtheorem{question}{Question}




\newtheorem{problem}{Problem}
%\renewcommand*{\theprob}{{\color{red}\arabic{section}.\arabic{prob}}}
\newtheorem{rprob}[problem]{\color{red} Problem}
%\renewcommand*{\thesprob}{{\color{red}\arabic{section}.\arabic{sprob}}}
% \newtheorem{ssprob}[prob]{$\star\star$ Problem}



\theoremstyle{plain}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{Conclusion}[definition]{Conclusion}
\newtheorem{thd}[definition]{Theorem-Definition}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{sthm}[definition]{$\star$ Theorem}
\newtheorem{slm}[definition]{$\star$ Lemma}
\newtheorem{claim}[definition]{Claim}
\newtheorem{spp}[definition]{$\star$ Proposition}
\newtheorem{scorollary}[definition]{$\star$ Corollary}


\newtheorem{condition}{Condition}
\newtheorem{Mthm}{Main Theorem}
\renewcommand{\thecondition}{\Alph{condition}} % "letter-numbered" theorems
\renewcommand{\theMthm}{\Alph{Mthm}} % "letter-numbered" theorems


%\substack   multiple lines under sum
%\underset{b}{a}   b is under a


% Remind: \overline{L_0}



\usepackage{calligra}
\DeclareMathOperator{\shom}{\mathscr{H}\text{\kern -3pt {\calligra\large om}}\,}
\DeclareMathOperator{\sext}{\mathscr{E}\text{\kern -3pt {\calligra\large xt}}\,}
\DeclareMathOperator{\Rel}{\mathscr{R}\text{\kern -3pt {\calligra\large el}~}\,}
\DeclareMathOperator{\sann}{\mathscr{A}\text{\kern -3pt {\calligra\large nn}}\,}
\DeclareMathOperator{\send}{\mathscr{E}\text{\kern -3pt {\calligra\large nd}}\,}
\DeclareMathOperator{\stor}{\mathscr{T}\text{\kern -3pt {\calligra\large or}}\,}
%write mathscr Hom (and so on) 

\usepackage{aurical}
\DeclareMathOperator{\VVir}{\text{\Fontlukas V}\text{\kern -0pt {\Fontlukas\large ir}}\,}

\newcommand{\vol}{\text{\Fontlukas V}}
\newcommand{\dvol}{d~\text{\Fontlukas V}}
% perfect Vol symbol

\usepackage{aurical}








\newcommand{\fk}{\mathfrak}
\newcommand{\mc}{\mathcal}
\newcommand{\wtd}{\widetilde}
\newcommand{\wht}{\widehat}
\newcommand{\wch}{\widecheck}
\newcommand{\ovl}{\overline}
\newcommand{\udl}{\underline}
\newcommand{\tr}{\mathrm{t}} %transpose
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\End}{\mathrm{End}} %endomorphism
\newcommand{\idt}{\mathbf{1}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\cond}[1]{\mathrm{cond}_{#1}}
\newcommand{\Conf}{\mathrm{Conf}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\res}{\mathrm{res}}
\newcommand{\KZ}{\mathrm{KZ}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\coev}{\mathrm{coev}}
\newcommand{\opp}{\mathrm{opp}}
\newcommand{\Rep}{\mathrm{Rep}}
\newcommand{\Dom}{\mathrm{Dom}}
\newcommand{\loc}{\mathrm{loc}}
\newcommand{\con}{\mathrm{c}}
\newcommand{\uni}{\mathrm{u}}
\newcommand{\ssp}{\mathrm{ss}}
\newcommand{\di}{\slashed d}
\newcommand{\Diffp}{\mathrm{Diff}^+}
\newcommand{\Diff}{\mathrm{Diff}}
\newcommand{\PSU}{\mathrm{PSU}(1,1)}
\newcommand{\Vir}{\mathrm{Vir}}
\newcommand{\Witt}{\mathscr W}
\newcommand{\Span}{\mathrm{Span}}
\newcommand{\pri}{\mathrm{p}}
\newcommand{\ER}{E^1(V)_{\mathbb R}}
\newcommand{\prth}[1]{( {#1})}
\newcommand{\bk}[1]{\langle {#1}\rangle}
\newcommand{\bigbk}[1]{\big\langle {#1}\big\rangle}
\newcommand{\Bigbk}[1]{\Big\langle {#1}\Big\rangle}
\newcommand{\biggbk}[1]{\bigg\langle {#1}\bigg\rangle}
\newcommand{\Biggbk}[1]{\Bigg\langle {#1}\Bigg\rangle}
\newcommand{\GA}{\mathscr G_{\mathcal A}}
\newcommand{\vs}{\varsigma}
\newcommand{\Vect}{\mathrm{Vec}}
\newcommand{\Vectc}{\mathrm{Vec}^{\mathbb C}}
\newcommand{\scr}{\mathscr}
\newcommand{\sjs}{\subset\joinrel\subset}
\newcommand{\Jtd}{\widetilde{\mathcal J}}
\newcommand{\gk}{\mathfrak g}
\newcommand{\hk}{\mathfrak h}
\newcommand{\xk}{\mathfrak x}
\newcommand{\yk}{\mathfrak y}
\newcommand{\zk}{\mathfrak z}
\newcommand{\pk}{\mathfrak p}
\newcommand{\hr}{\mathfrak h_{\mathbb R}}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\DHR}{\mathrm{DHR}_{I_0}}
\newcommand{\Repi}{\mathrm{Rep}_{\wtd I_0}}
\newcommand{\im}{\mathbf{i}}
\newcommand{\Co}{\complement}
%\newcommand{\Cu}{\mathcal C^{\mathrm u}}
\newcommand{\RepV}{\mathrm{Rep}^\uni(V)}
\newcommand{\RepA}{\mathrm{Rep}(\mathcal A)}
\newcommand{\RepN}{\mathrm{Rep}(\mathcal N)}
\newcommand{\RepfA}{\mathrm{Rep}^{\mathrm f}(\mathcal A)}
\newcommand{\RepAU}{\mathrm{Rep}^\uni(A_U)}
\newcommand{\RepU}{\mathrm{Rep}^\uni(U)}
\newcommand{\RepL}{\mathrm{Rep}^{\mathrm{L}}}
\newcommand{\HomL}{\mathrm{Hom}^{\mathrm{L}}}
\newcommand{\EndL}{\mathrm{End}^{\mathrm{L}}}
\newcommand{\Bim}{\mathrm{Bim}}
\newcommand{\BimA}{\mathrm{Bim}^\uni(A)}
%\newcommand{\shom}{\scr Hom}
\newcommand{\divi}{\mathrm{div}}
\newcommand{\sgm}{\varsigma}
\newcommand{\SX}{{S_{\fk X}}}
\newcommand{\DX}{D_{\fk X}}
\newcommand{\mbb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\bsb}{\boldsymbol}
\newcommand{\blt}{\bullet}
\newcommand{\Vbb}{\mathbb V}
\newcommand{\Ubb}{\mathbb U}
\newcommand{\Xbb}{\mathbb X}
\newcommand{\Kbb}{\mathbb K}
\newcommand{\Abb}{\mathbb A}
\newcommand{\Wbb}{\mathbb W}
\newcommand{\Mbb}{\mathbb M}
\newcommand{\Gbb}{\mathbb G}
\newcommand{\Cbb}{\mathbb C}
\newcommand{\Nbb}{\mathbb N}
\newcommand{\Zbb}{\mathbb Z}
\newcommand{\Qbb}{\mathbb Q}
\newcommand{\Pbb}{\mathbb P}
\newcommand{\Rbb}{\mathbb R}
\newcommand{\Ebb}{\mathbb E}
\newcommand{\Dbb}{\mathbb D}
\newcommand{\Hbb}{\mathbb H}
\newcommand{\cbf}{\mathbf c}
\newcommand{\Rbf}{\mathbf R}
\newcommand{\wt}{\mathrm{wt}}
\newcommand{\Lie}{\mathrm{Lie}}
\newcommand{\btl}{\blacktriangleleft}
\newcommand{\btr}{\blacktriangleright}
\newcommand{\svir}{\mathcal V\!\mathit{ir}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cok}{\mathrm{Coker}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\low}{\mathrm{low}}
\newcommand{\Sp}{\mathrm{Sp}}
\newcommand{\Rng}{\mathrm{Rng}}
\newcommand{\vN}{\mathrm{vN}}
\newcommand{\Ebf}{\mathbf E}
\newcommand{\Nbf}{\mathbf N}
\newcommand{\Stb}{\mathrm {Stb}}
\newcommand{\SXb}{{S_{\fk X_b}}}
\newcommand{\pr}{\mathrm {pr}}
\newcommand{\SXtd}{S_{\wtd{\fk X}}}
\newcommand{\univ}{\mathrm {univ}}
\newcommand{\vbf}{\mathbf v}
\newcommand{\ubf}{\mathbf u}
\newcommand{\wbf}{\mathbf w}
\newcommand{\CB}{\mathrm{CB}}
\newcommand{\Perm}{\mathrm{Perm}}
\newcommand{\Orb}{\mathrm{Orb}}
\newcommand{\Lss}{{L_{0,\mathrm{s}}}}
\newcommand{\Lni}{{L_{0,\mathrm{n}}}}
\newcommand{\UPSU}{\widetilde{\mathrm{PSU}}(1,1)}
\newcommand{\Sbb}{{\mathbb S}}
\newcommand{\Gc}{\mathscr G_c}
\newcommand{\Obj}{\mathrm{Obj}}
\newcommand{\bpr}{{}^\backprime}
\newcommand{\fin}{\mathrm{fin}}
\newcommand{\Ann}{\mathrm{Ann}}
\newcommand{\Real}{\mathrm{Re}}
\newcommand{\Imag}{\mathrm{Im}}
%\newcommand{\cl}{\mathrm{cl}}
\newcommand{\Ind}{\mathrm{Ind}}
\newcommand{\Supp}{\mathrm{Supp}}
\newcommand{\Specan}{\mathrm{Specan}}
\newcommand{\red}{\mathrm{red}}
\newcommand{\uph}{\upharpoonright}
\newcommand{\Mor}{\mathrm{Mor}}
\newcommand{\pre}{\mathrm{pre}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Jac}{\mathrm{Jac}}
\newcommand{\emb}{\mathrm{emb}}
\newcommand{\Sg}{\mathrm{Sg}}
\newcommand{\Nzd}{\mathrm{Nzd}}
\newcommand{\Owht}{\widehat{\scr O}}
\newcommand{\Ext}{\mathrm{Ext}}
\newcommand{\Tor}{\mathrm{Tor}}
\newcommand{\Com}{\mathrm{Com}}
\newcommand{\Mod}{\mathrm{Mod}}
\newcommand{\nk}{\mathfrak n}
\newcommand{\mk}{\mathfrak m}
\newcommand{\Ass}{\mathrm{Ass}}
\newcommand{\depth}{\mathrm{depth}}
\newcommand{\Coh}{\mathrm{Coh}}
\newcommand{\Gode}{\mathrm{Gode}}
\newcommand{\Fbb}{\mathbb F}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Modf}{\mathrm{Mod}^{\mathrm f}}
\newcommand{\codim}{\mathrm{codim}}
\newcommand{\card}{\mathrm{card}}
\newcommand{\dps}{\displaystyle}
\newcommand{\Int}{\mathrm{Int}}
\newcommand{\Nbh}{\mathrm{Nbh}}
\newcommand{\Pnbh}{\mathrm{PNbh}}
\newcommand{\Cl}{\mathrm{Cl}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\eps}{\varepsilon}
\newcommand{\Vol}{\mathrm{Vol}}
\newcommand{\LSC}{\mathrm{LSC}}
\newcommand{\USC}{\mathrm{USC}}
\newcommand{\Ess}{\mathrm{Rng}^{\mathrm{ess}}}
\newcommand{\Jbf}{\mathbf{J}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\Lin}{\mathrm{Lin}}
\newcommand{\ALin}{\mathrm{ALin}}
\newcommand{\bwn}{\bigwedge\nolimits}
\newcommand{\nbf}{\mathbf n}
\newcommand{\dive}{\mathrm{div}}




\usepackage{algorithm}
\usepackage{algorithmic}






\numberwithin{equation}{problem}
% count the eqation by section countation


\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\ran}{ran}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\img}{Im}
\DeclareMathOperator{\dd}{d\!}
\newcommand{\ie}{ \textit{ i.e. } }
\newcommand{\st}{ \textit{ s.t. }}


\usepackage[numbered,framed]{matlab-prettifier}
\lstset{
  style              = Matlab-editor,
  captionpos         =b,
  basicstyle         = \mlttfamily,
  escapechar         = ",
  mlshowsectionrules = true,
}

\usepackage[thehwcnt = 1]{iidef}
\thecourseinstitute{Tsinghua University}
\thecoursename{Discrete Optimistic}
\theterm{Fall 2024}
\hwname{Homework}
\usepackage{geometry}
\geometry{left=1.5cm,right=1.5cm,top=2.5cm,bottom=2.5cm}
\begin{document}
\courseheader
\name{Lin Zejin}
\rule{\textwidth}{1pt}
\begin{itemize}
\item {\bf Collaborators: \/}
  I finish this homework by myself. 
%   If you finish your homework all by yourself, make a similar statement. If you get help from others in finishing your homework, state like this:
%   \begin{itemize}
%   \item 1.2 (b) was solved with the help from \underline{\hspace{3em}}.
%   \item Discussion with \underline{\hspace{3em}} helped me finishing 1.3.
%   \end{itemize}
\end{itemize}
\rule{\textwidth}{1pt}

\vspace{2em}
 
\sloppy
\pagenumbering{arabic}

% put your code here
\begin{problem}
  By using contraction view, it suffices to show that the first step can  actually be viewed as using blue rules sequentially.

  For each node  $ n_i $, assume that  $ v_i $ is the edge connected to  $ n_i $ of minimal weight.
  
  WLOG, we assume  $ \omega(n_i),1 \leq i \geq |V| $  is increasing.
  
  Then we prove that coloring  $ v_i $ blue  simultaneously is equivalent to applying blue rule to the cut  $ (\{n_1,\cdots,n_k\},\{n_{k+1},\cdots,n_{|V|}\}) $ successively.

  By induction. For applying blue rule to the cut  $ (\{n_1\},\{n_2,\cdots,n_{|V|}\}) $,  $ v_1 $ turns blue since  $ v_1 $ is of the minimal weight in all  edges.
  
  If we have colored  $ v_1,\cdots,v_{k-1} $, then for applying blue rule to the cut  $ (\{n_1,\cdots,n_k\},\{n_{k+1},\cdots,n_{|V|}\}) $,  $ v_k $ turns blue. Otherwise, if  $ v_k'=(a,b) $ turns blue,  $ a\in \{n_1,\cdots,n_k\} $ and  $ b=n_t\in \{n_{k+1},\cdots,n_{|V|}\} $, then  $ \omega(v_k') \geq \omega(v_{t}) $ by the primitive rule. Thus  $ \omega(v_k') \geq \omega(v_t)  \geq \omega(v_k) $ as  $ t \geq k $, which causes contradiction.

  So actually we can apply blue rule one by one. Therefore,  $ Bor\overset{\circ    }{u}vka $ algorithm is indeed applying blue rule constantly, and it terminates until there is only one blue component, \ie forming a blue tree. As we proved in class, this tree is MST.
\end{problem}

\begin{problem}
  WLOG, we assume that  $ J_1,\cdots,J_n $  is exactly a possible schedule of minimal time cost.

  If  $ f_i  \leq  f_{i+1} $, then swap  $ J_i,J_{i+1} $. It will not affect other jobs. However, the time when job $ J_i,J_{i+1} $ finished both will be changed. It is   $ \max\{t+p_{i+1}+p_{i}+f_i,t+p_{i+1}+f_{i+1}\} $, which is not longer than  $ \max\{t+p_i+p_{i+1}+f_{i+1},t+p_i+f_i\}=t+p_i+p_{i+1}+f_{i+1} $ at first. So it is still  a possible schedule of minimal completion  time cost.
  
  Using bumbling sort algorithm, we can  rearrange  $ J_1,\cdots,J_n $ such that  $ f_i $ is decreasing, and it is still a possible schedule of minimal completion time cost.

  If  $ f_i=f_{i+1} $, the order will not affect the result obviously. 

  Since the schedule of minimal time cost must exist, it suffices to sort  $ f_i $ to get a possible schedule of minimal completion time cost. 
\end{problem}


\begin{problem}
  \begin{equation}\label{p3}
    \sum_{i=1}^n\omega_iC_i=\sum_{i=1}^n\omega_i\left(\sum_{j=1}^it_j\right)=(\sum_{i=1}^nt_i)(\sum_{i=1}^n\omega_i)-\sum_{i=1}^n\omega_i\left(\sum_{j=i+1}^nt_j\right)
  \end{equation}
  So it suffices to maximize  $ \dps\sum_{i=1}^n\omega_i\left(\sum_{j=i+1}^nt_j\right) $.

  WLOG, we assume that  $ (t_i,\omega_i),1 \leq i \leq n $ has been a possible solution.
  
  If  $ \dps\frac{\omega_i}{t_i}<\frac{\omega_{i+1}}{t_{i+1}}$, $ i<k $ 
  then swap  $ (\omega_i,t_i) $ and  $ (\omega_{k},t_{k}) $.   It only affects  $ i\sim k $-th terms  in the RHS of  \eqref{p3}. Noticed that the difference
  % \begin{align*}
  %   &\sum_{t=i}^k\omega_t\left(\sum_{j=t+1}^nt_j\right)-\left[\sum_{t=i+1}^{k-1}\omega_t\left(\sum_{j=t+1}^nt_j-t_k+t_i\right)+\omega_k\left(\sum_{j=i+1}^nt_j-t_k+t_i\right)+\omega_i\left(\sum_{j=k+1}^nt_j\right)\right]\\
  %   &=\sum_{t={i+1}}^{k-1}\omega_t(t_k-t_i)+\omega_i\sum_{j=i+1}^kt_j-\omega_k\sum_{j=i}^{k-1}t_j\\
  %   &=\omega_i\left(\sum_{j=i+1}^nt_j\right)-\omega_{i+1}\left(t_i+\sum_{j=i+2}^nt_j\right)\\
  %   & > 0
  % \end{align*}
  \begin{align*}
    \left[\omega_i\left(\sum_{j=i+1}^nt_j\right)+\omega_{i+1}\left(\sum_{j=i+2}^nt_j\right)\right]-\left[\omega_{i+1}\left(t_i+\sum_{j=i+2}^nt_j\right)+\omega_i\left(\sum_{j=i+2}^nt_j\right)\right]&=\omega_it_{i+1}-\omega_{i+1}t_i\\
    &=t_it_{i+1}(\frac{\omega_i}{t_i}-\frac{\omega_{i+1}}{t_{i+1}})<0
  \end{align*}
  So \eqref{p3} will be smaller as they swap, which contradicts that  \eqref{p3} has already taken the minimal value. 

  So a possible solution of minimal value must satisfy that  $ \dps\frac{\omega_i}{t_i} $ is decreasing.
  
  If  $ \dps\frac{\omega_i}{t_i}=\frac{\omega_{i+1}}{t_{i+1}} $, \eqref{p3} remains the same as they swap. So a possible solution of minimal value  is a rearrangement that satisfy   $ \dps\frac{\omega_i}{t_i} $ is decreasing.

  So it suffices to sort  $ \dps\frac{\omega_i}{t_i} $ decreasingly.
\end{problem}
\begin{problem}
  The problem can be reduced to the original case.
  
  We can enumerate each job. Take the first job, and remove other jobs that are not compatible with the first job. Here we remove the interval of the first job, and hence get a line of jobs. Apply the original algorithm of interval scheduling to the rest of jobs and we get a number that maximize the numbers of compatible jobs in the case that the first job is taken. Compare such  $ n $  numbers we can obtain the answer. 
\end{problem}
\begin{problem}
  Use a algorithm similar to Dijkstra's algorithm.

  Denote  $ s $  as the place they start.
  \begin{algorithm}
    \caption{Algorithm}
    \label{alg:dijkstra}
    \begin{algorithmic}[1]
      \STATE   
      $ \begin{cases}
        d[x]\leftarrow f_{(s,x)}(0),\mathrm{Pred}[x]=s,&(s,x)\in E\\
        d[x]\leftarrow+\infty,&\text{otherwise}
      \end{cases}  $ 
      \STATE  $ S\leftarrow\{s\} $ 
      \STATE  $ d[s]\leftarrow 0 $ 
    \WHILE{$ S\neq V $}
        \STATE Choose  $ u\in \dps\arg\min_{x\not\in S}\{d[x]\} $.\label{Choose u min}
        \STATE Update  $ S\leftarrow S\cup\{u\} $.\label{1}
        \FOR{each $ x\in V-S $,$ (u,x)\in E $}\label{2}
            \IF{ $ d[u]+f_{(u,x)}(d[u])<d[x] $ }
                \STATE  $ d[x]\leftarrow d[u]+f_{(u,x)}(d[u]) $  
                \STATE  $ \mathrm{Pred}[x]\leftarrow u $  
            \ENDIF
        \ENDFOR 
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}
We prove by induction to  $ |S| $   that every  $ d[u] $ is the minimal cost time from  $ s $ to  $ u $.   
\begin{proof}
  For  $ |S|=1 $, trivial.
  At each step in \ref{1},  let  $ v=\mathrm{Pred}[u] $. Then  $ d[u]=d[v]+f_{(v,u)}(d[v]) $.

  By the algorithm step \ref{2},  $ d[u] $ is the minimal cost time from  $ s $ to  $ u $ as we restrict the graph to  $ S $.   
  
  If there is a path $ P $  that cost less than  $ d[u] $, then the path pass through points outside  $ S $. Let  $ \alpha\to \beta $ in this path that  $ \alpha\in S $,  $ \beta\not\in S $. Then  $ \alpha\neq u $ and  
  \begin{align*}
    \mathrm{length}(P)& \geq \mathrm{length}(P[s\rightarrow\beta])\\
    & =\mathrm{length}(P[s\rightarrow\alpha])+f_{\alpha\to \beta}(d[\alpha])\
    & \geq d[\alpha]+f_{\alpha\to \beta}(d[\alpha])\qquad(\text{by inductive hypothesis})\\
    & \geq d[\beta] \geq d[u]\qquad(\text{by step \ref{Choose u min}})
  \end{align*}

  So  $ d[u] $ is the minimal cost time from  $ s $ to  $ u $ in  $ G $.    
\end{proof}
\end{problem}
\begin{problem}
  Use Boruvka's Algorithm. After  $ i $-th  WHILE iterations, with the contraction view, $  |V_i| \leq \dps\frac{|V|}{2^{i-1}}  $,  $ |E_i| \leq |V_i|+8 \in O(|V_i|)$.
  
  Then the time complexity  $ \dps\sum|E_i| \leq \dps\sum O(\frac{|V|}{2^{i-1}}) \leq O(|V|) $ 
\end{problem}
\begin{problem}
  \begin{enumerate}[label=(\alph*)]
    \item Since all edge costs are distinct, applying blue rule or red rule is the unique operation at any time. Therefore, the final graph with a blue tree is unique. If MST is not unique, there exists a MST that is not the blue tree. WLOG we assume  $ T $ is not the blue tree completely. Choose a blue edge  $ e\not\in T $. Then for  its corresponding fundamental cycle   $ C $, since any cutset contains at least two edges in  $ C $,  $ \exists e'\in C\cap T $ \st  $ e' $ and  $ e $ are in the same cutset where  we color  $ e $ blue by applying blue rule $ \Rightarrow $      $ \omega(e') \geq \omega(e) $. However, by the result in lecture,  $ \omega(e') \leq \omega(e) $ $ \Rightarrow  $  $ \omega(e')=\omega(e) $ $ \Rightarrow  $ contradiction!      So  MST can only be the blue tree.
    \item Yes, the proof is below:
    \begin{proof}
      For each MST  $ T $ in  $ G $, choose  $ \epsilon>0 $ small enough such that  $\forall e\in E, [\omega(e)-\epsilon,\omega(e))\cup(\omega(e),\omega(e)+\epsilon] $ will not contains any of  $ \omega(e),e\in E $.
      
      Take some changes whose rule set is:
      \begin{enumerate}[label=(\arabic*)]
        \item If  $ T=\{e_1,e_2,\cdots,e_t\} $ where  $ \omega(e_1)  \geq  \omega(e_2)  \geq  \cdots  \geq  \omega(e_t) $, replace  $ \omega(e_i) $ with  $ \omega(e_i)-\frac{i}{t}\epsilon $. Then after the changes,  $ \omega(e_1)>\omega(e_2)>\cdot>\omega(e_t) $.  
        \item   After (1), if  $ \exists \omega(e_1)=\omega(e_2)=\cdots=\omega(e_k) $, where  $ e_i\in E $ distinct and  $ k $ is the largest number, then we replace  $ \omega(e_i) $ with  $ \omega(e_i)+\frac{i}{k}\epsilon $.     
      \end{enumerate}
      Step (1)(2) makes sure that after the change, there are no edges that have the same cost. So in this case the MST is unique. Now we prove this unique MST is exactly the original one,  $ T $.
      
      First, after (1),  $ \omega(e),e\in T $ will not be same as the cost of other edges, so they will never change after (2). Therefore, the cost of  $ T $  decreases  $ \dps\sum_{i=1}^t\frac{i}{t}$. Moerover, the cost of  any tree in  $ G $ has less decreased or even increased. So  $ T $ is still the MST.    

      Finally, we prove that apply Kruskal's Algorithm to the changed graph produces a same valid execution of Kruskal's Algorithm on the original $ G $.

      That's because, the change will not change the ordinal relationship between edges with distinct cost. So the effect of the change on the order is only to rearrange the edges with the same weight. So the effect of the change on the order is only to rearrange the edges with the same weight. Equivalently, the order of edges in Kruskal's Algorithm on the changed graph is still a valid order in Kruskal's Algorithm on  the original graph. Since Kruskal's Algorithm returns the same result if the order and the edges are determined, they produces the same MST,  $ T $. 
    \end{proof}

    \item The output graph  $ H $ have no cycle of 3 or 4 edges. Or equivalenty, it has no triangles and squares. That's because, 
    if it has a cycle of  $ k $ edges,  $ k \leq 4 $, then assume  $ e  $ is the last edge we put into the graph. Then  $ e $ is the longest edge so  $ l<kl_e<4l_e $ .  Let  $ l $ be the length of the cycle. Then     $ 3l_e <l-l_e  $. But  $ l<4l_e $ $ \Rightarrow  $ contradiction!

    Since  $ H $  has no triangles, it has to be a bipartite graph $ (V_1,V_2) $ where all edges connect points in  $ V_1 $ and  $ V_2 $. Let  $ c_v,v\in V_1 $ be the degree of  $ v $ and  $ S_v $ be the set of  $ v'\in V_2 $  connected to  $ v $. Since there is no squares in  $ H $,  $ |S_{v_1}\cap S_{v_2}| \leq 1,\,\forall v_1,v_2\in V_1,v_1\neq v_2 $.
    
    Therefore, each pair  $ \{u_1,u_2\}\subset V_2 $ is contained by one  $ S_v $ at most. $ S_v $ contains  $ \dps\frac{c_v(c_v-1)}{2} $ such a pair   So we have 
    \[\sum_{v\in V_1}\frac{c_v(c_v-1)}{2} \leq \frac{|V_2|(|V_2|-1)}{2}\]
    Then by Cauchy's inequallity, 
    \[\sum_{v\in V_1}c_v \leq |V_1|+\sqrt{|V_1|\cdot\sum_{v\in V_1}(c_v-1)^2} \leq n+\sqrt{|V_1|\cdot |V_2|(|V_2|-1)} \leq n+\sqrt{\frac{n^2}{4}(|V_2|-1)} \leq 2025n^{\frac{3}{2}} \]
    So the number of edges  $ f(n) \dps\sum_{v\in V_1}c_v \leq 2025n^{\frac{3}{2}} $. Hence  $ \dps\lim_{n\to\infty}\frac{f(n)}{n^2}=0 $   
  \end{enumerate}
\end{problem}

\begin{problem}
  Since it contains no cycles, we can determine the root at first.

  Also because it contains no cycle, each edge in  minimum-cost arborescence is some cheap edges, otherwise one can replace  it with the cheap edge, it stil has no directed cycles and each node  $ v\neq r$ has exactly one incoming edge, hence remains a arborescence.

  So this arborescence $ A $ contains all cheap edges, so it should be minimum-cost arborescence as we proved in the lecture. 
\end{problem}
\end{document}